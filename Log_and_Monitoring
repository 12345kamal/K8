-->Log and Monitoring<--

##Clone the metrics server repository:##
git clone https://github.com/linuxacademy/metrics-server

##Install the metrics server in your cluster:##
kubectl apply -f ~/metrics-server/deploy/1.8+/

##Get a response from the metrics server API:##
kubectl get --raw /apis/metrics.k8s.io/

##Get the CPU and memory utilization of the nodes in your cluster:##
kubectl top node

##Get the CPU and memory utilization of the pods in your cluster:##
kubectl top pods

##Get the CPU and memory of pods in all namespaces:##
kubectl top pods --all-namespaces

##Get the CPU and memory of pods in only one namespace:##
kubectl top pods -n kube-system

##Get the CPU and memory of pods with a label selector:##
kubectl top pod -l run=pod-with-defaults

##Get the CPU and memory of a specific pod:##
kubectl top pod pod-with-defaults

##Get the CPU and memory of the containers inside the pod:##
kubectl top pods group-context --containers

##The pod YAML for a liveness probe:##
apiVersion: v1
kind: Pod
metadata:
  name: liveness
spec:
  containers:
  - image: linuxacademycontent/kubeserve
    name: kubeserve
    livenessProbe:
      httpGet:
        path: /
        port: 80

##Check if the failed pod has been added to the list of endpoints:##
kubectl get ep

##The directory where the container logs reside:##
/var/log/containers

##The directory where kubelet stores its logs:##
/var/log

##The YAML for a pod that has two different log streams:##
apiVersion: v1
kind: Pod
metadata:
  name: counter
spec:
  containers:
  - name: count
    image: busybox
    args:
    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$i: $(date)" >> /var/log/1.log;
        echo "$(date) INFO $i" >> /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  volumes:
  - name: varlog
    emptyDir: {}

##View the logs in the /var/log directory of the container:##
kubectl exec counter -- ls /var/log

##The YAML for a sidecar container that will tail the logs for each type:##
apiVersion: v1
kind: Pod
metadata:
  name: counter
spec:
  containers:
  - name: count
    image: busybox
    args:
    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$i: $(date)" >> /var/log/1.log;
        echo "$(date) INFO $i" >> /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: count-log-1
    image: busybox
    args: [/bin/sh, -c, 'tail -n+1 -f /var/log/1.log']
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: count-log-2
    image: busybox
    args: [/bin/sh, -c, 'tail -n+1 -f /var/log/2.log']
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  volumes:
  - name: varlog
    emptyDir: {}

##View the first type of logs separately:##
kubectl logs counter count-log-1

##Get the logs from a specific container on a pod:##
kubectl logs counter -c count-log-1

##Get the logs from all containers on the pod:##
kubectl logs counter --all-containers=true

##Get the logs from containers with a certain label:##
kubectl logs -lapp=nginx

##Get the logs from a previously terminated container within a pod:##
kubectl logs -p -c nginx nginx

##Stream the logs from a container in a pod:##
kubectl logs -f -c count-log-1 counter

##Tail the logs to only view a certain number of lines:##
kubectl logs --tail=20 nginx

##View the logs from a previous time duration:##
kubectl logs --since=1h nginx

##View the logs from a container within a pod within a deployment:##
kubectl logs deployment/nginx -c nginx

##Redirect the output of the logs to a file:##
kubectl logs counter -c count-log-1 > count.log


-->Troubleshooting<---

##The YAML for a pod with a termination reason:##
apiVersion: v1
kind: Pod
metadata:
  name: pod2
spec:
  containers:
  - image: busybox
    name: main
    command:
    - sh
    - -c
    - 'echo "I''ve had enough" > /var/termination-reason ; exit 1'
    terminationMessagePath: /var/termination-reason

##One of the first steps in troubleshooting is usually to describe the pod:##
kubectl describe po pod2

##The YAML for a liveness probe that checks for pod health:##
apiVersion: v1
kind: Pod
metadata:
  name: liveness
spec:
  containers:
  - image: linuxacademycontent/candy-service:2
    name: kubeserve
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8081

##View the logs for additional detail:##
kubectl logs pod-with-defaults

##Export the YAML of a running pod, in the case that you are unable to edit it directly:##
kubectl get po pod-with-defaults -o yaml --export > defaults-pod.yaml

##Edit a pod directly (i.e., changing the image):##
kubectl edit po nginx

##Check the events in the kube-system namespace for errors:##
kubectl get events -n kube-system

##Get the logs from the individual pods in your kube-system namespace and check for errors:##
kubectl logs [kube_scheduler_pod_name] -n kube-system

##Check the status of the Docker service:##
sudo systemctl status docker

##Start up and enable the Docker service, so it starts upon bootup:##
sudo systemctl enable docker && systemctl start docker

##Start up and enable the kubelet service, so it starts up when the machine is rebooted:##
sudo systemctl enable kubelet && systemctl start kubelet

##Turn off swap on your machine:##
sudo su -
swapoff -a && sed -i '/ swap / s/^/#/' /etc/fstab

##Check if you have a firewall running:##Disable the firewall and stop the firewalld service:##
sudo systemctl disable firewalld && systemctl stop firewalld

##Generate a new token after spinning up a new server:##
sudo kubeadm token generate

##Create the kubeadm join command for your new worker node:##
sudo kubeadm token create [token_name] --ttl 2h --print-join-command

##View the journalctl logs:##
sudo journalctl -u kubelet

##View the syslogs:##
sudo more syslog | tail -120 | grep kubelet

##Run a deployment using the container port 9376 and with three replicas:##
kubectl run hostnames --image=k8s.gcr.io/serve_hostname \
                        --labels=app=hostnames \
                        --port=9376 \
                        --replicas=3

##List the services in your cluster:##
kubectl get svc

##Create a service by exposing a port on the deployment:##
kubectl expose deployment hostnames --port=80 --target-port=9376

##Run an interactive busybox pod:##
kubectl run -it --rm --restart=Never busybox --image=busybox:1.28 sh

##From the pod, check if DNS is resolving hostnames:##
# nslookup hostnames

##From the pod, cat out the /etc/resolv.conf file:##
# cat /etc/resolv.conf

##From the pod, look up the DNS name of the Kubernetes service:##
# nslookup kubernetes.default
